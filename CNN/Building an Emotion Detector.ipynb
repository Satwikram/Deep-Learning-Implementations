{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Emotion Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28273 images belonging to 6 classes.\n",
      "Found 3534 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "\n",
    "num_classes = 6\n",
    "img_rows, img_cols = 48, 48\n",
    "batch_size = 64\n",
    "\n",
    "train_data_dir = './fer2013_1/train'\n",
    "validation_data_dir = './fer2013_1/validation'\n",
    "\n",
    "# Let's use some data augmentaiton \n",
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=30,\n",
    "      shear_range=0.3,\n",
    "      zoom_range=0.3,\n",
    "      width_shift_range=0.4,\n",
    "      height_shift_range=0.4,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    " \n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    " \n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        color_mode = 'grayscale',\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True)\n",
    " \n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        color_mode = 'grayscale',\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Keras Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import ELU\n",
    "from tensorflow.keras.layers import Activation, Flatten, Dropout, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras LittleVGG Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 48, 48, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 48, 48, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 48, 48, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 48, 48, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 24, 24, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 24, 24, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 24, 24, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 12, 12, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 12, 12, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 6, 6, 256)         295168    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 6, 6, 256)         590080    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                147520    \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 390       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 1,328,102\n",
      "Trainable params: 1,325,926\n",
      "Non-trainable params: 2,176\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding = 'same', kernel_initializer=\"he_normal\",\n",
    "                 input_shape = (img_rows, img_cols, 1)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, (3, 3), padding = \"same\", kernel_initializer=\"he_normal\", \n",
    "                 input_shape = (img_rows, img_cols, 1)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Block #2: second CONV => RELU => CONV => RELU => POOL\n",
    "# layer set\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Block #3: third CONV => RELU => CONV => RELU => POOL\n",
    "# layer set\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Block #4: third CONV => RELU => CONV => RELU => POOL\n",
    "# layer set\n",
    "model.add(Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Block #5: first set of FC => RELU layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Block #6: second set of FC => RELU layers\n",
    "model.add(Dense(64, kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Block #7: softmax classifier\n",
    "model.add(Dense(num_classes, kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop, SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "                     \n",
    "checkpoint = ModelCheckpoint(\"emotions.h5\",\n",
    "                             monitor=\"val_loss\",\n",
    "                             mode=\"min\",\n",
    "                             save_best_only = True,\n",
    "                             verbose=1)\n",
    "\n",
    "earlystop = EarlyStopping(monitor = 'val_loss', \n",
    "                          min_delta = 0, \n",
    "                          patience = 3,\n",
    "                          verbose = 1,\n",
    "                          restore_best_weights = True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.2, patience = 3, verbose = 1, min_delta = 0.0001)\n",
    "\n",
    "# we put our call backs into a callback list\n",
    "callbacks = [earlystop, checkpoint, reduce_lr]\n",
    "\n",
    "# We use a very small learning rate \n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = Adam(lr=0.001),\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "nb_train_samples = 28273\n",
    "nb_validation_samples = 3534\n",
    "epochs = 10\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch = nb_train_samples // batch_size,\n",
    "    epochs = epochs,\n",
    "    callbacks = callbacks,\n",
    "    validation_data = validation_generator,\n",
    "    validation_steps = nb_validation_samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "my_model = load_model('emotions.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import RMSprop, SGD, Adam\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retraining from last epoch where it stopped!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint1 = ModelCheckpoint(\"emotions.h5\",\n",
    "                             monitor=\"val_loss\",\n",
    "                             mode=\"min\",\n",
    "                             save_best_only = True,\n",
    "                             verbose=1)\n",
    "\n",
    "earlystop1 = EarlyStopping(monitor = 'val_loss', \n",
    "                          min_delta = 0, \n",
    "                          patience = 3,\n",
    "                          verbose = 1,\n",
    "                          restore_best_weights = True)\n",
    "\n",
    "reduce_lr1 = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.2, patience = 3, verbose = 1, min_delta = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks1 = [earlystop1, checkpoint1, reduce_lr1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_train_samples = 28273\n",
    "nb_validation_samples = 3534\n",
    "epochs = 10\n",
    "\n",
    "history = my_model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch = nb_train_samples // batch_size,\n",
    "    epochs = epochs,\n",
    "    callbacks = callbacks1,\n",
    "    validation_data = validation_generator,\n",
    "    validation_steps = nb_validation_samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "emotion = load_model('emotions.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3534 images belonging to 6 classes.\n",
      "Confusion Matrix\n",
      "[[274  25  15  78  81  18]\n",
      " [106  80  23  77 130 112]\n",
      " [ 24   4 773  32  22  24]\n",
      " [101  21 152 146 118  88]\n",
      " [ 65  21  40 153 304  11]\n",
      " [ 16  19  20  16   5 340]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Angry       0.47      0.56      0.51       491\n",
      "        Fear       0.47      0.15      0.23       528\n",
      "       Happy       0.76      0.88      0.81       879\n",
      "     Neutral       0.29      0.23      0.26       626\n",
      "         Sad       0.46      0.51      0.48       594\n",
      "    Surprise       0.57      0.82      0.67       416\n",
      "\n",
      "    accuracy                           0.54      3534\n",
      "   macro avg       0.50      0.53      0.49      3534\n",
      "weighted avg       0.52      0.54      0.51      3534\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "nb_train_samples = 28273\n",
    "nb_validation_samples = 3534\n",
    "\n",
    "# We need to recreate our validation generator with shuffle = false\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        color_mode = 'grayscale',\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "\n",
    "class_labels = validation_generator.class_indices\n",
    "class_labels = {v: k for k, v in class_labels.items()}\n",
    "classes = list(class_labels.values())\n",
    "\n",
    "#Confution Matrix and Classification Report\n",
    "Y_pred = emotion.predict_generator(validation_generator, nb_validation_samples // batch_size+1)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(validation_generator.classes, y_pred))\n",
    "print('Classification Report')\n",
    "target_names = list(class_labels.values())\n",
    "print(classification_report(validation_generator.classes, y_pred, target_names=target_names))\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "cnf_matrix = confusion_matrix(validation_generator.classes, y_pred)\n",
    "\n",
    "plt.imshow(cnf_matrix, interpolation='nearest')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(classes))\n",
    "_ = plt.xticks(tick_marks, classes, rotation=90)\n",
    "_ = plt.yticks(tick_marks, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get our class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3534 images belonging to 6 classes.\n",
      "{0: 'Angry', 1: 'Fear', 2: 'Happy', 3: 'Neutral', 4: 'Sad', 5: 'Surprise'}\n"
     ]
    }
   ],
   "source": [
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        color_mode = 'grayscale',\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "\n",
    "class_labels = validation_generator.class_indices\n",
    "class_labels = {v: k for k, v in class_labels.items()}\n",
    "classes = list(class_labels.values())\n",
    "print(class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's test on some of validation images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import RMSprop, SGD, Adam\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "\n",
    "def draw_test(name, pred, im, true_label):\n",
    "    BLACK = [0,0,0]\n",
    "    expanded_image = cv2.copyMakeBorder(im, 160, 0, 0, 300 ,cv2.BORDER_CONSTANT,value=BLACK)\n",
    "    cv2.putText(expanded_image, \"predited - \"+ pred, (20, 60) , cv2.FONT_HERSHEY_SIMPLEX,1, (0,0,255), 2)\n",
    "    cv2.putText(expanded_image, \"true - \"+ true_label, (20, 120) , cv2.FONT_HERSHEY_SIMPLEX,1, (0,255,0), 2)\n",
    "    cv2.imshow(name, expanded_image)\n",
    "\n",
    "\n",
    "def getRandomImage(path, img_width, img_height):\n",
    "    \"\"\"function loads a random images from a random folder in our test path \"\"\"\n",
    "    folders = list(filter(lambda x: os.path.isdir(os.path.join(path, x)), os.listdir(path)))\n",
    "    random_directory = np.random.randint(0,len(folders))\n",
    "    path_class = folders[random_directory]\n",
    "    file_path = path + path_class\n",
    "    file_names = [f for f in listdir(file_path) if isfile(join(file_path, f))]\n",
    "    random_file_index = np.random.randint(0,len(file_names))\n",
    "    image_name = file_names[random_file_index]\n",
    "    final_path = file_path + \"/\" + image_name\n",
    "    return image.load_img(final_path, target_size = (img_width, img_height),grayscale=True), final_path, path_class\n",
    "\n",
    "# dimensions of our images\n",
    "img_width, img_height = 48, 48\n",
    "\n",
    "# We use a very small learning rate \n",
    "emotion.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = RMSprop(lr = 0.001),\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "files = []\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "# predicting images\n",
    "for i in range(0, 10):\n",
    "    path = './fer2013_1/validation/' \n",
    "    img, final_path, true_label = getRandomImage(path, img_width, img_height)\n",
    "    files.append(final_path)\n",
    "    true_labels.append(true_label)\n",
    "    x = image.img_to_array(img)\n",
    "    x = x * 1./255\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    images = np.vstack([x])\n",
    "    classes = emotion.predict_classes(images, batch_size = 10)\n",
    "    predictions.append(classes)\n",
    "    \n",
    "for i in range(0, len(files)):\n",
    "    image = cv2.imread((files[i]))\n",
    "    image = cv2.resize(image, None, fx=3, fy=3, interpolation = cv2.INTER_CUBIC)\n",
    "    draw_test(\"Prediction\", class_labels[predictions[i][0]], image, true_labels[i])\n",
    "    cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on a single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "\n",
    "face_classifier = cv2.CascadeClassifier('./Haarcascades/haarcascade_frontalface_default.xml')\n",
    "\n",
    "def face_detector(img):\n",
    "    # Convert image to grayscale\n",
    "    gray = cv2.cvtColor(img.copy(),cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
    "    if faces is ():\n",
    "        return (0,0,0,0), np.zeros((48,48), np.uint8), img\n",
    "    \n",
    "    allfaces = []   \n",
    "    rects = []\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_gray = cv2.resize(roi_gray, (48, 48), interpolation = cv2.INTER_AREA)\n",
    "        allfaces.append(roi_gray)\n",
    "        rects.append((x,w,y,h))\n",
    "    return rects, allfaces, img\n",
    "\n",
    "img = cv2.imread(\"rajeev.jpg\")\n",
    "rects, faces, image = face_detector(img)\n",
    "\n",
    "i = 0\n",
    "for face in faces:\n",
    "    roi = face.astype(\"float\") / 255.0\n",
    "    roi = img_to_array(roi)\n",
    "    roi = np.expand_dims(roi, axis=0)\n",
    "\n",
    "    # make a prediction on the ROI, then lookup the class\n",
    "    preds = emotion.predict(roi)[0]\n",
    "    label = class_labels[preds.argmax()]   \n",
    "\n",
    "    #Overlay our detected emotion on our pic\n",
    "    label_position = (rects[i][0] + int((rects[i][1]/2)), abs(rects[i][2] - 10))\n",
    "    i =+ 1\n",
    "    cv2.putText(image, label, label_position , cv2.FONT_HERSHEY_SIMPLEX,1, (0,255,0), 2)\n",
    "    \n",
    "cv2.imshow(\"Emotion Detector\", image)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try this on our webcam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-3825113a3ab1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mrect\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mface\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mface_detector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mface\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mroi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mface\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"float\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m255.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-3825113a3ab1>\u001b[0m in \u001b[0;36mface_detector\u001b[1;34m(img)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m# Convert image to grayscale\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mgray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mfaces\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mface_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetectMultiScale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfaces\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m48\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m48\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "\n",
    "face_classifier = cv2.CascadeClassifier('./Haarcascades/haarcascade_frontalface_default.xml')\n",
    "\n",
    "def face_detector(img):\n",
    "    # Convert image to grayscale\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
    "    if faces is ():\n",
    "        return (0,0,0,0), np.zeros((48,48), np.uint8), img\n",
    "    \n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "\n",
    "    try:\n",
    "        roi_gray = cv2.resize(roi_gray, (48, 48), interpolation = cv2.INTER_AREA)\n",
    "    except:\n",
    "        return (x,w,y,h), np.zeros((48,48), np.uint8), img\n",
    "    return (x,w,y,h), roi_gray, img\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    rect, face, image = face_detector(frame)\n",
    "    if np.sum([face]) != 0.0:\n",
    "        roi = face.astype(\"float\") / 255.0\n",
    "        roi = img_to_array(roi)\n",
    "        roi = np.expand_dims(roi, axis=0)\n",
    "\n",
    "        # make a prediction on the ROI, then lookup the class\n",
    "        preds = emotion.predict(roi)[0]\n",
    "        label = class_labels[preds.argmax()]  \n",
    "        label_position = (rect[0] + int((rect[1]/2)), rect[2] + 25)\n",
    "        cv2.putText(image, label, label_position , cv2.FONT_HERSHEY_SIMPLEX,2, (0,255,0), 3)\n",
    "    else:\n",
    "        cv2.putText(image, \"No Face Found\", (20, 60) , cv2.FONT_HERSHEY_SIMPLEX,2, (0,255,0), 3)\n",
    "        \n",
    "    cv2.imshow('All', image)\n",
    "    if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
